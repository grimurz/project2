{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Web Traffic Analysis\n",
    "**This is the second of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-11-10, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project your task is to analyze a stream of log entries. A log entry consists of an [IP address](https://en.wikipedia.org/wiki/IP_address) and a [domain name](https://en.wikipedia.org/wiki/Domain_name). For example, a log line may look as follows:\n",
    "\n",
    "`192.168.0.1 somedomain.dk`\n",
    "\n",
    "One log line is the result of the event that the domain name was visited by someone having the corresponding IP address. Your task is to analyze the traffic on a number of domains. Counting the number of unique IPs seen on a domain doesn't correspond to the exact number of unique visitors, but it is a good estimate.\n",
    "\n",
    "Specifically, you should answer the following questions from the stream of log entries.\n",
    "\n",
    "- How many unique IPs are there in the stream?\n",
    "- How many unique IPs are there for each domain?\n",
    "- How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "\n",
    "**The answers to these questions can be approximate!**\n",
    "\n",
    "You should also try to answer one or more of the following, more advanced, questions. The answers to these should also be approximate.\n",
    "\n",
    "- How many unique IPs are there for the domains $d_1, d_2, \\ldots$?\n",
    "- How many times was IP X seen on domains $d_1, d_2, \\ldots$?\n",
    "- What are the X most frequent IPs in the stream?\n",
    "\n",
    "You should use algorithms and data structures that you've learned about in the lectures, and you should provide your own implementations of these.\n",
    "\n",
    "Furthermore, you are expected to:\n",
    "\n",
    "- Document the accuracy of your answers when using algorithms that give approximate answers\n",
    "- Argue why you are using certain parameters for your data structures\n",
    "\n",
    "This notebook is in three parts. In the first part you are given an example of how to read from the stream (which for the purpose of this project is a remote file). In the second part you should implement the algorithms and data structures that you intend to use, and in the last part you should use these for analyzing the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the stream\n",
    "The following code reads a remote file line by line. It is wrapped in a generator to make it easier to extend. You may modify this if you want to, but your solution should remain parametrized, so that your notebook can be run without having to consume the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def stream(n):\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic_2?linkToken=_DcyO-U3MjjuNzI-&itemName=traffic_2') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\")\n",
    "            yield element\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.99.192.116\tpython.org\n",
      "202.152.82.171\twikipedia.org\n",
      "130.126.231.205\tpython.org\n",
      "116.142.112.214\tpandas.pydata.org\n",
      "113.124.204.127\tpython.org\n",
      "143.30.183.87\twikipedia.org\n",
      "138.74.228.219\tpython.org\n",
      "56.120.106.87\twikipedia.org\n",
      "189.119.55.225\twikipedia.org\n",
      "180.110.73.101\twikipedia.org\n"
     ]
    }
   ],
   "source": [
    "STREAM_SIZE = 10\n",
    "web_traffic_stream = stream(STREAM_SIZE)\n",
    "for i in web_traffic_stream:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "\n",
    "try:\n",
    "    # try with a fast c-implementation ...\n",
    "    import mmh3 as mmh3\n",
    "except ImportError:\n",
    "    # ... otherwise fallback to this code!\n",
    "    import pymmh3 as mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique IPs in stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_SIZE_a = 10000\n",
    "web_traffic_stream = stream(STREAM_SIZE_a)\n",
    "\n",
    "#test_test = []\n",
    "\n",
    "upper_part_len = 8\n",
    "m = pow(2,upper_part_len) # number of bins\n",
    "ht = [0] * m\n",
    "\n",
    "bit_len = 32\n",
    "alpha_dict = {16:0.673, 32:0.697, 64:0.709} # bias constants\n",
    "\n",
    "for i in web_traffic_stream:\n",
    "    \n",
    "    ip = i.split()[0]\n",
    "    h = mmh3.hash(ip, seed=555)\n",
    "    b = '{:032b}'.format(h)\n",
    "    \n",
    "#    print(b)\n",
    "\n",
    "    upper_part = b[:upper_part_len]\n",
    "    lower_part = b[upper_part_len:]   \n",
    "    \n",
    "    leading_zeros = len(lower_part.split('1', 1)[0])\n",
    "    \n",
    "    bin_idx = int(upper_part, 2)\n",
    "    ht[bin_idx] = max(ht[bin_idx],leading_zeros)    \n",
    "    \n",
    "#    print(upper_part,lower_part,leading_zeros,int(upper_part, 2))\n",
    "    \n",
    "#    if ip not in test_test:\n",
    "#        test_test.append(ip)\n",
    "#    else:\n",
    "#        print(ip)\n",
    "\n",
    "#print(len(test_test))\n",
    "#print(test_test)\n",
    "#print(ht)\n",
    "#print(alpha_dict[bit_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique IPs seen on each domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_SIZE_c = 50000\n",
    "web_traffic_stream = stream(STREAM_SIZE_c)\n",
    "\n",
    "dom_dict = {}\n",
    "\n",
    "upper_part_len = 8\n",
    "m = pow(2,upper_part_len) # number of bins\n",
    "\n",
    "for i in web_traffic_stream:\n",
    "    \n",
    "    ip = i.split()[0]\n",
    "    dom = i.split()[1]\n",
    "\n",
    "    h = mmh3.hash(ip, seed=555)\n",
    "    b = '{:032b}'.format(h)\n",
    "\n",
    "    upper_part = b[:upper_part_len]\n",
    "    lower_part = b[upper_part_len:]   \n",
    "    \n",
    "    leading_zeros = len(lower_part.split('1', 1)[0])\n",
    "    \n",
    "    bin_idx = int(upper_part, 2)\n",
    "    \n",
    "    if dom not in dom_dict.keys():\n",
    "        dom_dict[dom] = [0] * m\n",
    "        \n",
    "    dom_dict[dom][bin_idx] = max(dom_dict[dom][bin_idx],leading_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of IPs seen on each domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length and number of hash tables (4x8 and 8x128)\n",
    "dom_ht_size = 8\n",
    "dom_ht_no = 4\n",
    "ip_ht_size = 128\n",
    "ip_ht_no = 8\n",
    "\n",
    "# Initialize count and reference hash tables for domain\n",
    "dom_ht = np.zeros( (dom_ht_no, dom_ht_size) )\n",
    "dom_ht_ref = np.zeros( (dom_ht_no, dom_ht_size, ip_ht_no, ip_ht_size) )\n",
    "\n",
    "## Initialize count hash table for IP\n",
    "#ip_ht = np.zeros( (ip_ht_no, ip_ht_size) )\n",
    "\n",
    "seeds = [2,28,280,2800,8008,800,80,8, 1,2,3,4,5,6,7,8]\n",
    "    \n",
    "STREAM_SIZE_b = 50000\n",
    "web_traffic_stream = stream(STREAM_SIZE_b)\n",
    "\n",
    "for i in web_traffic_stream:\n",
    "    \n",
    "    ip = i.split()[0]\n",
    "    dom = i.split()[1]\n",
    "\n",
    "    dom_indices = [mmh3.hash(dom, seed=s) % dom_ht_size for s in seeds[0:dom_ht_no]]\n",
    "    ip_indices = [mmh3.hash(ip, seed=s) % ip_ht_size for s in seeds[0:ip_ht_no]]\n",
    "\n",
    "    for j,dom_idx in enumerate(dom_indices):\n",
    "        dom_ht[j][dom_idx] += 1\n",
    "        \n",
    "        for k,ip_idx in enumerate(ip_indices):\n",
    "#            ip_ht[k][ip_idx] += 1\n",
    "            dom_ht_ref[j][dom_idx][k][ip_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many unique IPs are there in the stream?\n",
    "Here we used Hyperloglog to keep track of the number of unique IPs. The count algorithm was obtained from Wikipedia and according to the article the error should only be around 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 4492 unique IPs in a stream of 10000\n"
     ]
    }
   ],
   "source": [
    "sum_tot = 0\n",
    "\n",
    "for i in ht:\n",
    "    sum_tot += pow(2,-i)\n",
    "    \n",
    "Z = pow(sum_tot,-1)\n",
    "\n",
    "print('Approximately',round(alpha_dict[bit_len] * pow(m,2) * Z), 'unique IPs in a stream of', STREAM_SIZE_a)\n",
    "\n",
    "#ht[128] = round(st.mean(ht))\n",
    "#import statistics as st\n",
    "#test = st.harmonic_mean(ht)\n",
    "#print(ht[126:131],ht[128])\n",
    "#print('Approximately',m,test,m*test)\n",
    "#print(len(ht))\n",
    "#print(round(st.mean(ht)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many unique IPs are there for each domain?\n",
    "It turns out there are only about 10 different domains so storing a hash table for each of them in a dictionary should be adequate. The same Hyperloglog algorithm was used as in previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 6347 unique IPs on python.org in a stream of 50000\n",
      "Approximately 10772 unique IPs on wikipedia.org in a stream of 50000\n",
      "Approximately 3075 unique IPs on pandas.pydata.org in a stream of 50000\n",
      "Approximately 690 unique IPs on dtu.dk in a stream of 50000\n",
      "Approximately 708 unique IPs on google.com in a stream of 50000\n",
      "Approximately 403 unique IPs on databricks.com in a stream of 50000\n",
      "Approximately 382 unique IPs on github.com in a stream of 50000\n",
      "Approximately 249 unique IPs on spark.apache.org in a stream of 50000\n",
      "Approximately 202 unique IPs on datarobot.com in a stream of 50000\n",
      "Approximately 179 unique IPs on scala-lang.org in a stream of 50000\n"
     ]
    }
   ],
   "source": [
    "for dom,ht in dom_dict.items():\n",
    "\n",
    "    sum_tot = 0\n",
    "\n",
    "    for i in ht:\n",
    "        sum_tot += pow(2,-i)\n",
    "\n",
    "    Z = pow(sum_tot,-1)\n",
    "\n",
    "    print('Approximately',round(alpha_dict[bit_len] * pow(m,2) * Z), 'unique IPs on '+dom+' in a stream of', STREAM_SIZE_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "This was implemented by creating a CountMin hash table (dom_ht) to keep track of the number of times a domain appeared. Another four dimensional table (dom_ht_ref) was used as a domain CountMin hash table of IP CountMin hash tables. This way it is possible to keep track of the times an IP appeard on a domain. dom_ht determines the row with the least collisions and consequently which index to use in dom_ht_ref.\n",
    "\n",
    "The number of unique domains appears to be rather low and that is why four hash tables of length 8 should suffice and be accurate enough. Increasing the number of hash tables for IP from 8 to 16 results in several counters being equal or very close to the lowest value. Therefore, eigth hash tables of length 128 seem to be enough for IP.\n",
    "\n",
    "The arrangement of having a hash table of hash tables is perhaps somewhat wasteful of space but it seems to be accurate. In hindsight, the number of unique domains is so low that a simple dictionary would probably suffice. In any case, our method is better equipped to deal with a drastic increase in unique domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times 56.120.106.87 was seen on wikipedia.org in a stream of 50000: 178\n"
     ]
    }
   ],
   "source": [
    "# Retrieve hash table indices for 'wikipedia'\n",
    "dom_indices = [mmh3.hash('wikipedia.org', seed=s) % dom_ht_size for s in seeds[0:dom_ht_no]]\n",
    "\n",
    "# Find the index of domain corrisponding to the lowest count\n",
    "dom_count = [int(j[dom_indices[i]]) for i,j in enumerate(dom_ht)]\n",
    "dom_count_idx_min = dom_count.index(min(dom_count))\n",
    "\n",
    "# Retrieve hash table indices for '56.120.106.87'\n",
    "ip_indices = [mmh3.hash('56.120.106.87', seed=s) % ip_ht_size for s in seeds[0:ip_ht_no]]\n",
    "\n",
    "# Find lowest count of the ip\n",
    "ip_count = [int(j[ip_indices[i]]) for i,j in enumerate(dom_ht_ref[dom_count_idx_min][dom_indices[dom_count_idx_min]])]\n",
    "print('Number of times 56.120.106.87 was seen on wikipedia.org in a stream of',str(STREAM_SIZE_b)+':',min(ip_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dom_indices:  [3, 0, 3, 5]\n",
      "dom_count:  [25991, 26647, 27336, 26097]\n",
      "dom_count_idx_min:  0\n",
      "ip_indices:  [33, 65, 72, 35, 7, 100, 75, 33]\n",
      "ip_count:  [245, 205, 224, 199, 178, 178, 189, 186]\n"
     ]
    }
   ],
   "source": [
    "print('dom_indices: ',dom_indices)\n",
    "print('dom_count: ',dom_count)\n",
    "print('dom_count_idx_min: ',dom_count_idx_min)\n",
    "print('ip_indices: ',ip_indices)\n",
    "print('ip_count: ',ip_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many unique IPs are there for the domains  d1,d2,… ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 6347 unique IPs on python.org in a stream of 50000\n",
      "Approximately 10772 unique IPs on wikipedia.org in a stream of 50000\n",
      "Approximately 3075 unique IPs on pandas.pydata.org in a stream of 50000\n",
      "Approximately 690 unique IPs on dtu.dk in a stream of 50000\n"
     ]
    }
   ],
   "source": [
    "domains = ['wikipedia.org','python.org','pandas.pydata.org','dtu.dk']\n",
    "\n",
    "for dom,ht in dom_dict.items():\n",
    "\n",
    "    if dom in domains:\n",
    "        sum_tot = 0\n",
    "\n",
    "        for i in ht:\n",
    "            sum_tot += pow(2,-i)\n",
    "\n",
    "        Z = pow(sum_tot,-1)\n",
    "\n",
    "        print('Approximately',round(alpha_dict[bit_len] * pow(m,2) * Z), 'unique IPs on '+dom+' in a stream of', STREAM_SIZE_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many times was IP X seen on domains  d1,d2,… ?\n",
    "Handled the same way as before but within a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times 56.120.106.87 was seen on wikipedia.org, python.org, pandas.pydata.org and dtu.dk in a stream of 50000: 319\n"
     ]
    }
   ],
   "source": [
    "ip = '56.120.106.87'\n",
    "domains = ['wikipedia.org','python.org','pandas.pydata.org','dtu.dk']\n",
    "ip_count_tot = 0\n",
    "\n",
    "for dom in domains:\n",
    "\n",
    "    # Retrieve hash table indices for the domain\n",
    "    dom_indices = [mmh3.hash(dom, seed=s) % dom_ht_size for s in seeds[0:dom_ht_no]]\n",
    "\n",
    "    # Find the index of domain corrisponding to the lowest count\n",
    "    dom_count = [int(j[dom_indices[i]]) for i,j in enumerate(dom_ht)]\n",
    "    dom_count_idx_min = dom_count.index(min(dom_count))\n",
    "\n",
    "    # Retrieve hash table indices for the ip\n",
    "    ip_indices = [mmh3.hash(ip, seed=s) % ip_ht_size for s in seeds[0:ip_ht_no]]\n",
    "\n",
    "    # Find lowest count of the ip\n",
    "    ip_count = [int(j[ip_indices[i]]) for i,j in enumerate(dom_ht_ref[dom_count_idx_min][dom_indices[dom_count_idx_min]])]\n",
    "    ip_count_tot += min(ip_count)\n",
    "\n",
    "print('Number of times '+ip+' was seen on',', '.join(domains[0:-1]),'and',domains[-1],'in a stream of',str(STREAM_SIZE_b)+':',ip_count_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the X most frequent IPs in the stream?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
